{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPGUC4f56zhhwTRX6jf+6qZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crap89/IT_ACADEMY/blob/main/Tarea_M5_T02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios de Train-Test con ScikitLearn a partir de un dataset con información sobre las viviendas de Houston."
      ],
      "metadata": {
        "id": "PJEdnFPv43nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1:\n",
        "\n",
        "Parte el conjunto de datos adjunto en train y test. Estudia ambos conjuntos por separado, a nivel descriptivo.\n",
        "\n",
        "También adjunto encontrarás una descripción de las diferentes variables del dataset.\n"
      ],
      "metadata": {
        "id": "XLPUlml9478h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaLCKtn540lN"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías que vamos a necesitar:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos los datos con los que vamos a trabajar:\n",
        "df_housing = pd.read_csv(\"housing data.csv\", header=None)\n",
        "df_housing.head()"
      ],
      "metadata": {
        "id": "9x8xlziq5R1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asignamos los nombres a las columnas:\n",
        "df_housing.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "df_housing.head()"
      ],
      "metadata": {
        "id": "ST2Hcerk6En4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descripción de los atributos.\n",
        "\n",
        "Estudio del dataset."
      ],
      "metadata": {
        "id": "3JjPlsiX6m3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dimensiones del dataset\n",
        "df_housing.shape"
      ],
      "metadata": {
        "id": "SeCqa9jI6ZXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tipos de las variables:\n",
        "df_housing.dtypes"
      ],
      "metadata": {
        "id": "gNZogYNO6jQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# búsqueda de valores vacíos:\n",
        "df_housing.isna().sum()"
      ],
      "metadata": {
        "id": "YwI6yzlQ7ImD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# búsqueda de valores nulos:\n",
        "df_housing.isnull().sum()"
      ],
      "metadata": {
        "id": "OreOUHhy7Nlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing.describe().round(2).T"
      ],
      "metadata": {
        "id": "wf8kHmvp7RaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis previo de las variables del dataset.\n",
        "\n",
        "Si bien vamos a centrarnos en analizar el subdataset sobre el que vamos a construir nuestro modelo (conjunto de datos Train), ver profundidad cada variable, sus relaciones entre ellas y las relaciones con la variable objetivo, haremos un análisis previo de las variables del dataset completo. Con este paso previo queremos ver si alguna variable no tiene relación ninguna con nuestra variable resultado, o si tienen relaciones de correlación que no aporten nada al estudio."
      ],
      "metadata": {
        "id": "zAwRdADL7bVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relación entre variables del dataset completo:"
      ],
      "metadata": {
        "id": "q6AzSVKd7hxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df_housing);"
      ],
      "metadata": {
        "id": "u47z_xGf7U7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlación entre variables del dataset completo.\n",
        "\n",
        "Analizamos la correlación entre variables para ver si hay algunas que estén muy correlacionasdas y pueda afectar a nuestro modelo. También queremos ver si alguna de las variables atributo no está nada correlacionada con nuestra variable objetivo y no aporta nada al estudio."
      ],
      "metadata": {
        "id": "7CLDWpyz8BBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df_housing.corr()\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(correlation_matrix, cbar=True, fmt='.1f', annot=True, cmap='Blues');"
      ],
      "metadata": {
        "id": "0oQjDRFy79IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La información que extraemos de esta primera aproximación al dataset completo, junto con el resumen estadístico previo, es la siguiente:\n",
        "\n",
        "\n",
        "\n",
        "*   La tasa de criminalidad (CRIM) y la proporción de suelo residencial (ZN)   aportan pocos datos, la mayoría son cero. \n",
        "*   La variable índice de acceso a la autopistas (RAD) es numérica ordinal.\n",
        "*   La variable extensión del las orillas del río (CHAS) es binaria.\n",
        "*   Salvo la distribución del valor medio de las viviendas (MEDV) y el número de habitaciones por vivienda (RM), el resto de variables tienen una distribución fuertemente asimétrica (cuartiles desequilibrados y desviaciones estardart altas) o tienen distribución no normal, necesitarán ser preprocesadas.\n",
        "*   La mayorías de los datos están muy dispersos.\n",
        "*  Vemos algunas correlaciones claras, lineales o polinómicas, entre estas variables:\n",
        "     ◘ MEDV con RM y LSTAT\n",
        "     ◘ LSTAT con NOX, RM, AGE, DIS, MEDV\n",
        "     ◘ DIS con NOX, AGE, LSTAT, MEDV\n",
        "     ◘ AGE con NOX, DIS, LSTAT, MEDV\n",
        "     ◘ RM con LSTAT Y MEDV\n",
        "     ◘ NOX con RM, AGE, DIS, LSTAT, MEDV\n",
        "     ◘ CRIM con AGE, DIS, LSTST, MEDV\n",
        "\n",
        "• No hay ninguna variable fuertemente correlacionada con nuestra variable objetivo y que influya negativamente en el estudio.\n",
        "\n"
      ],
      "metadata": {
        "id": "T4vncA_I8RS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definimos nuestra variable objetivo/target.\n",
        "\n",
        "A partir de este dataset queremos construir un modelo que nos permita predecir el valor medio de las casas ocupadas por sus propietarios (MEDV) en función de las características de la zona, esta será por tanto, nuestra variable respuesta."
      ],
      "metadata": {
        "id": "5eAp5dmV9cW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_housing['MEDV'].copy()\n",
        "y.shape"
      ],
      "metadata": {
        "id": "WQZuUOXx8QrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el dataset que contiene los atributos de nuestro estudio:\n",
        "X = df_housing.drop('MEDV', axis=1).copy()\n",
        "X.shape"
      ],
      "metadata": {
        "id": "lwKstWR18LTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos nuestro dataset para poder contruir nuestro modelo. Hacemos dos conjuntos, Train para entrenar nuestro modelo, y Test, para testear su eficacia. Tienen una proporción de 67% y 33% respectivamente."
      ],
      "metadata": {
        "id": "kTONFpQd9rLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "ubl3_03H9vU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estudio de la variable objetivo \"Valor medio de las casas ocupadas por sus propietarios por cada 1000$\"."
      ],
      "metadata": {
        "id": "pHfY4WaE9zYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudio de la distribución.\n",
        "\n",
        "Nuestra variable objetivo es numérica continua. Vemos qué distribución tiene originalmente y en cada conjunto Train y Test después de dividir nuestro dataset."
      ],
      "metadata": {
        "id": "9x4aaFTc95LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(df_housing['MEDV'], label='Distribución original', color='blue', fill=True)\n",
        "sns.kdeplot(y_train, label='Distribución conjunto Train', color='red')\n",
        "sns.kdeplot(y_test, label='Distribución conjunto Test', color='orange')\n",
        "plt.title(\"Distribución original y distribuciones Train-Test\", fontsize = 14)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "ZWBx8BGZ9yCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que tanto el conjunto Train como el conjunto Test tienen una distribución similar al conjunto original, no hace falta aplicar la técnica Data Shuffling para repartir mejor los datos."
      ],
      "metadata": {
        "id": "CHMKL0a_-AO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A continuación queremos saber si tiene una distribución normal. Planteamos las hipótesis:\n",
        "\n",
        "Hipótesis nula H0, nuestra muestra tiene una distribución normal.               \n",
        "Hipótesis H1, nuestra muestra no tiene una distribución normal.\n",
        "\n",
        "Para validar nuestra hipótesis utilizamos el Test de distribución gaussiana Shapiro-Wilk con un alpha del 5%."
      ],
      "metadata": {
        "id": "sc9NYBa_-IGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "def distribucion_normal(data):\n",
        "    stat, p = shapiro(data)\n",
        "    print('stat=%.3f, p-value=%.20f' % (stat, p))\n",
        "    if p > 0.05:\n",
        "        print('Aceptamos la hipótesis nula H0: Probablemente Gaussiana')\n",
        "    else:\n",
        "        print('Rechazamos la hipótesis nula H0, nos quedamos con la alternativa H1: Probablemente no Gaussiana')"
      ],
      "metadata": {
        "id": "Jlvxzr6v-CxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribucion_normal(y)"
      ],
      "metadata": {
        "id": "m2zV_onp-alb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estudio de la simetría, la forma y los outliers:\n",
        "sns.boxplot(x = y)\n",
        "plt.title(\"Análisis de Outliers\", fontsize = 14);"
      ],
      "metadata": {
        "id": "jNXSrur--b6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewness = y.skew()\n",
        "kurtosis = y.kurtosis()\n",
        "print(f'Kurtosis= {kurtosis}', f'Skewness= {skewness}' )"
      ],
      "metadata": {
        "id": "YHlw7Ln2-1dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.describe()"
      ],
      "metadata": {
        "id": "dcwNOO3U-6Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora sabemos que nuestro target tiene una distribución no normal, asimétrica, con una asimetría positva (skewness con valores positivos). La curtosis positiva indica que tiene valores atípicos y que tiene una cola más alargada, la derecha. Es una variable con outliers y con poca dispersión.\n",
        "\n",
        "Necesita un preprocesado con Robust Scaler"
      ],
      "metadata": {
        "id": "sx7vr9Tw--L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analizamos los atributos o variables independientes.\n",
        "\n",
        "Para analizar cada atributo vamos a hacer una gráfica con sus distribuciones en el dataset original y después de dividir el dataset, tanto en el conjunto Train como en el conjunto Test. Para analizar los outliners hacemos un boxplot.\n",
        "\n",
        "Hacemos un dataframe con los nombres de las columnas a estudiar para poder poner título a los gráficos con más comodidad."
      ],
      "metadata": {
        "id": "izZ8u6BJ_EW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_atributos = pd.DataFrame(\n",
        "    {'columnas':['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT'],  \n",
        "     'definiciones':['Tasa de criminalidad per capita por ciudad', 'Proporción de suelo residencial cada 25.000 pies cuadrados', 'Proporción de negocios mayoristas por acres', 'Concentración de óxido nitrítico (partes por 10 millones)', 'Media del número de habitaciones por vivienda', 'Proporción de viviendas contruidas antes de 1940 y habitadas por sus propietarios', 'Distancias ponderadas a cinco centros de empleos de Boston', 'Impuesto sobre el valor total de la propiedad por cada 10.000$', 'Ratio de alumnos por profesor', '1000(Bk - 0.63)^2 siendo Bk la proporción de ciudadanos negros por ciudad', 'Porcentaje de población bajo nivel adquisitivo']})\n",
        "df_atributos.set_index('columnas', inplace=True)\n",
        "df_atributos.head()"
      ],
      "metadata": {
        "id": "akGHQWfr-_Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # hacemos una lista con los atributos que vamos a analizar:\n",
        "atributos = list(df_atributos.index)  "
      ],
      "metadata": {
        "id": "jorQw4lh_Rxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analisis_atributo(colum):\n",
        "    print('Análisis del atributo', df_atributos.definiciones[colum])\n",
        "    print('')\n",
        "    print('Análisis estadístico')\n",
        "    # análisis estadístico\n",
        "    print(X[colum].describe())\n",
        "    print('')\n",
        "    skewness = X[colum].skew()\n",
        "    kurtosis = X[colum].kurtosis()\n",
        "    print(f'Kurtosis= {kurtosis}')\n",
        "    print(f'Skewness= {skewness}')\n",
        "    print('')\n",
        "    print('Test de normalidad')\n",
        "    distribucion_normal(X_train[colum])\n",
        "    print('')\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
        "    axes = axes.flat    \n",
        "    # análisis de la distibución\n",
        "    sns.kdeplot(X[colum], label='Distribución original', color='gray', fill=True, ax = axes[0])\n",
        "    sns.kdeplot(X_train[colum], label='Distribución conjunto Train', color='red', ax = axes[0])\n",
        "    sns.kdeplot(X_test[colum], label='Distribución conjunto Test', color='blue', ax = axes[0])    \n",
        "    axes[0].set_title(\"Distribución original (gris) y distribuciones Train (rojo) y Test (azul)\", fontsize = 16)\n",
        "    axes[0].set_xlabel(df_atributos.definiciones[colum], fontsize= 16) \n",
        "    axes[0].tick_params(labelsize = 10)\n",
        "    print('')\n",
        "    # análisis de los outliers\n",
        "    sns.boxplot(x = X_train[colum])\n",
        "    axes[1].set_title(\"Análisis de Outliers en el conjunto Train\", fontsize = 16)\n",
        "    axes[1].set_xlabel(df_atributos.definiciones[colum], fontsize = 16) \n",
        "    axes[1].tick_params(labelsize = 16)\n",
        "    # análisis de la relación con la variable objetivo\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 5))\n",
        "    sns.regplot(\n",
        "        x           = y_train,\n",
        "        y           = X_train[colum],\n",
        "        color       = \"black\",\n",
        "        marker      = 'o',\n",
        "        scatter_kws = {\"alpha\":0.7},\n",
        "        line_kws    = {\"color\":\"r\",\"alpha\":0.7},\n",
        "        ax          = axes\n",
        "    )\n",
        "    axes.set_title(f\"Valor medio de las casas ocupadas por sus propietarios por cada 1000$ y {df_atributos.definiciones[colum]}\", fontsize = 16)\n",
        "    axes.tick_params(labelsize = 12)\n",
        "    axes.set_xlabel(\"Valor medio de las casas ocupadas por sus propietarios por cada 1000$\", fontsize = 16)"
      ],
      "metadata": {
        "id": "xff8_9m3_YyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'CRIM'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "d7Oare5p_jWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Tasa de criminalidad tiene una distribución unimodal no normal con una gran asimetría positiva, valor alto de skewness, debido a que la mayoría de sus valores son cero. También vemos una curtosis alta porque tiene valores atípicos y outliers. El resto de valores están muy dispersos.\n",
        "La relación con la variable objetivo es lineal, la mayor criminalidad se da en zonas donde el valor medio de las viviendas es menor y a partir de cierto valor (alrededor de 25.000$) es casi inexistente."
      ],
      "metadata": {
        "id": "1jKZcpGN_wpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos un preprocesado con Robust Scaler para controlar los outliers."
      ],
      "metadata": {
        "id": "SVsZLn_I_zmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'ZN'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "ekWOMDk1_o6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La distribución de la variable Proporción de suelo residencial es unimodal no normal, asimétrica positiva, con outliers.\n",
        "\n",
        "La mayoría de los datos son cero, como si no se hubiera valorado en la mayoría de registros, el resto de datos no tienen una relación con una pauta con nuestra variable. Queda excluida en este ejercicio de nuestro modelo."
      ],
      "metadata": {
        "id": "hIWyJMJb_7mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'INDUS'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "pUJnlGXp_5nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable Proporción de negocios mayoristas tiene una distribución bimodal no gaussiana, con alta dispersión en sus valores, sin outliers."
      ],
      "metadata": {
        "id": "B4nSfq2GAF_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos una normalización con StandardScaler:"
      ],
      "metadata": {
        "id": "bTM6htVoAIGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'NOX'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "l2gi4jScAMUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable Concentración de óxido nitrítico tiene una distribución unimodal no gaussiana, con alta dispersión en sus valores, sin outliers."
      ],
      "metadata": {
        "id": "D0Jbhdo0ASDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos una normalización con MinMaxScaler:"
      ],
      "metadata": {
        "id": "8oQH0qTAAU53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'RM'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "2YTJ3_t_AZhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Media del número de habitaciones por vivienda tiene una distribución unimodal no normal con una kurtosis acentuada que refleja valores atípicos. Estos valores se observan en la gráficas como outliers y como un grupo de viviendas con alto valor, independientemente del número de habitaciones, pueden indicar un mala toma de datos.\n",
        "\n",
        "La relación con la variable objetivo es evidente y lineal, a mayor número de habitaciones el valor medio de las viviendas aumenta."
      ],
      "metadata": {
        "id": "No1OcXAxAfqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos un preprocesado con Robust Scaler para tratar los valores atípicos."
      ],
      "metadata": {
        "id": "Qwq0RC9bAt8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'AGE'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "FA2jsCtfAfaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La distribución de la proporción de viviendas construidas antes de 1940 tiene una distribución bimodal no gaussiana con asimetría negativa y curtosis negativa, valores atípicos sin outliers y una alta dispersión."
      ],
      "metadata": {
        "id": "pnx6K212A13x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos una normalización con MinMaxScaler."
      ],
      "metadata": {
        "id": "BL_Jdf6DA3k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'DIS'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "bQF-rP0JA9TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La distribución de la variable Distancias ponderadas a cinco centros de empleo es unimodal no normal, asimétrica positiva, con algún outlier.\n",
        "\n",
        "Observamos que tiene relación con nuestra variable objetivo, a menos distancia menos valor medio."
      ],
      "metadata": {
        "id": "KpvBB5cSBDrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos un preprocesado con Robust Scaler:"
      ],
      "metadata": {
        "id": "pgJwfvogBFsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'TAX'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "Dk5EnziIBKJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impuesto sobre el valor total de la propiedad tiene una distribución bimodal no gaussiana, con alta dispersión en sus valores, sin outliers."
      ],
      "metadata": {
        "id": "Crkwx_MJBQas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos una normalización con MinMax Scaler."
      ],
      "metadata": {
        "id": "FCM9tAoXBRYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'PTRATIO'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "C2q9oiiNBU4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable Ratio de alumno por profesor no tiene relación con nuestra variable objetivo, la dejamos fuera en este ejercicio del estudio de nuestro modelo."
      ],
      "metadata": {
        "id": "5NY4n4yyBZUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'B'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "0T_PVQP2BcdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Proporción de ciudadanos negros tiene una distribución unimodal asimétrica con una gran grupo de los valores en torno a un mismo rango de entre 300 y 400. Tiene outliers."
      ],
      "metadata": {
        "id": "81KZLiOZBhHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proponemos un preprocesado con Robust Scaler."
      ],
      "metadata": {
        "id": "ap1rB6WLBjD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colum = 'LSTAT'\n",
        "analisis_atributo(colum)"
      ],
      "metadata": {
        "id": "awkwSOVjBlvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La distribucion de la variable Nivel adquisito es unimodal no normal, con outliers.\n",
        "Tiene una fuerte relación lineal con el Valor medio de la vivienda.\n",
        "\n",
        "Proponemos un preprocesados con Robust Scaler."
      ],
      "metadata": {
        "id": "AFjsxCxmBrq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Ejercicio 2:\n",
        "\n",
        "Aplica algún proceso de transformación (estandarizar los datos numéricos, crear columnas dummies, polinomios...)."
      ],
      "metadata": {
        "id": "Asm0nVObByjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un dataset para filtrar las columnas con las que no vamos a trabajar y hacer las transformaciones.\n",
        "X_trainFS = X_train.drop(['ZN','PTRATIO'] , axis=1).copy()\n",
        "X_testFS = X_test.drop(['ZN','PTRATIO'] , axis=1).copy()"
      ],
      "metadata": {
        "id": "zDWR3RXbB9bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesado de nuestros datos, Feature Scaling:\n",
        "\n",
        "Ya tenemos el estudio de todas nuestras variables, vamos a preprocesarlas. El preprocesado debe hacerse para ambos conjuntos con los que vamos a entrenar y validar nuestro modelo."
      ],
      "metadata": {
        "id": "RhYDlLWLCHAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler"
      ],
      "metadata": {
        "id": "eWadGVgiCEci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variable Índice de accesibilidad a las autopistas radiales:\n",
        "\n",
        "Esta variable es numérica ordinal, sólo puede tomar un rango de valores. Aplicamos un proceso de dummificación."
      ],
      "metadata": {
        "id": "ykuvbPq8CO0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# valores únicos de RED\n",
        "X_trainFS.RAD.unique()"
      ],
      "metadata": {
        "id": "5ZeHdvXHCSp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trainFS.RAD.nunique() "
      ],
      "metadata": {
        "id": "RFtKKD6oCXYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # total de registros por valor\n",
        "X_trainFS.RAD.value_counts()"
      ],
      "metadata": {
        "id": "KJYAApSbCdzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conjunto Train"
      ],
      "metadata": {
        "id": "xPmITNcZClbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos las columnas dummies\n",
        "dummy_RAD = pd.get_dummies(X_trainFS[\"RAD\"], prefix=\"RAD\")\n",
        "# eliminamos la variable RAD\n",
        "X_trainFS = X_trainFS.drop([\"RAD\"], axis = 1)\n",
        "# Añadimos las columnas dummies creadas al dataset original.\n",
        "X_trainFS = pd.concat([X_trainFS, dummy_RAD], axis = 1)\n",
        "X_trainFS.head()"
      ],
      "metadata": {
        "id": "tkL4ep1nCmqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conjunto Test:"
      ],
      "metadata": {
        "id": "ip6lvSYYCrjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos las columnas dummies\n",
        "dummy_RAD = pd.get_dummies(X_testFS[\"RAD\"], prefix=\"RAD\")\n",
        "# eliminamos la variable RAD\n",
        "X_testFS = X_testFS.drop([\"RAD\"], axis = 1)\n",
        "# Añadimos las columnas dummies creadas al dataset original.\n",
        "X_testFS = pd.concat([X_testFS, dummy_RAD], axis = 1)\n",
        "X_testFS.head()"
      ],
      "metadata": {
        "id": "Cr-S0e2uCpYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variable Extensión de las orilas del Río Charles.\n",
        "\n",
        "Esta variable es una variable dummie, no necesita preprocesado.\n",
        "\n",
        "Variables con distribución no normal sin outliers, preprocesado para normalizar las distribuciones con MinMaxScaler.\n",
        "\n",
        "Dataset Train."
      ],
      "metadata": {
        "id": "ApsQ5zZDCxcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variablesNorm = ['INDUS', 'AGE', 'TAX', 'NOX']\n",
        "mms = MinMaxScaler(feature_range = (0, 1))\n",
        "X_trainFS[variablesNorm] = mms.fit_transform(X_train[variablesNorm])"
      ],
      "metadata": {
        "id": "hfUaljxWC7e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Test:"
      ],
      "metadata": {
        "id": "TsNEHO8LDAKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_testFS[variablesNorm] = mms.transform(X_test[variablesNorm])"
      ],
      "metadata": {
        "id": "fD7TGUdRDCCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables con valores atípicos y ourliers, preprocesado de Robust Scaler.\n",
        "\n",
        "Dataset Train."
      ],
      "metadata": {
        "id": "s96dFgbzDGcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variablesRbSc = ['CRIM', 'RM', 'DIS', 'LSTAT','B']\n",
        "rs = RobustScaler()\n",
        "X_trainFS[variablesRbSc] = rs.fit_transform(X_train[variablesRbSc])"
      ],
      "metadata": {
        "id": "DU_Np4OGDGOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Test:\n",
        "X_testFS[variablesRbSc] = rs.transform(X_test[variablesRbSc])"
      ],
      "metadata": {
        "id": "xL0bAKQsDNWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3:\n",
        "\n",
        "Resume las nuevas columnas generadas de forma estadística y gráfica"
      ],
      "metadata": {
        "id": "dwhwdcmtDUFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_graf = [X_train['INDUS'],X_train['NOX'],X_train['CRIM'],X_train['RM'],X_train['DIS'],X_train['TAX'],X_train['B'],X_train['LSTAT']]\n",
        "fig = plt.figure(1, figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "bp = ax.boxplot(datos_graf)\n",
        "ax.set_title(\"Variables antes del preprocesado\", fontsize = 12);"
      ],
      "metadata": {
        "id": "MeIOsuwMDYL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_grafFS = [X_trainFS['INDUS'],X_trainFS['NOX'],X_trainFS['CRIM'],X_trainFS['RM'],X_trainFS['DIS'],X_trainFS['TAX'],X_trainFS['B'],X_trainFS['LSTAT']]\n",
        "fig = plt.figure(1, figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "bp = ax.boxplot(datos_grafFS)\n",
        "ax.set_title(\"Variables después del preprocesado\", fontsize = 12);"
      ],
      "metadata": {
        "id": "OjdsAaeGDf8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras el preprocesado las distribuciones están centradas y los outliers preparados para poder trabajar.\n",
        "\n",
        "Nuestro dataset queda preparado para poder definir un modelo y entrenarlo."
      ],
      "metadata": {
        "id": "ZuanIto9DiGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_trainFS.head()"
      ],
      "metadata": {
        "id": "y3vMEqBNDmXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_testFS.head()"
      ],
      "metadata": {
        "id": "bO8mmis8DqXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}